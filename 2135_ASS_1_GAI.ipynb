{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz3SI1qOnZDhq8xGvowxAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52135/Generative-AI/blob/main/2135_ASS_1_GAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "YActual YP red\n",
        "20 20.5\n",
        "30 30.3\n",
        "40 40.2\n",
        "50 50.6\n",
        "60 60.7\n",
        "Tabela 1: YActual Vs. YP red\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uzjpUrhPIzOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGH9hfV4-t_1",
        "outputId": "507d4549-ac3f-43dc-a152-93e251c0c69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual MAE: 0.4600000000000016\n",
            "Manual MSE: 0.24600000000000147\n",
            "Manual RMSE: 0.49598387070549127\n",
            "\n",
            "Comparison with Sklearn Metrics:\n",
            "Sklearn MAE: 0.4600000000000016\n",
            "Sklearn MSE: 0.24600000000000147\n",
            "Sklearn RMSE: 0.49598387070549127\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "\n",
        "# Actual and Predicted Values\n",
        "y_actual = np.array([20, 30, 40, 50, 60])\n",
        "y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "# 1. Mean Absolute Error (MAE)\n",
        "mae = np.mean(np.abs(y_actual - y_pred))\n",
        "print(f\"Manual MAE: {mae}\")\n",
        "\n",
        "# 2. Mean Squared Error (MSE)\n",
        "mse = np.mean((y_actual - y_pred)**2)\n",
        "print(f\"Manual MSE: {mse}\")\n",
        "\n",
        "# 3. Root Mean Squared Error (RMSE)\n",
        "rmse = math.sqrt(mse)\n",
        "print(f\"Manual RMSE: {rmse}\")\n",
        "\n",
        "# Comparison with Sklearn Metrics\n",
        "sklearn_mae = mean_absolute_error(y_actual, y_pred)\n",
        "sklearn_mse = mean_squared_error(y_actual, y_pred)\n",
        "sklearn_rmse = math.sqrt(sklearn_mse)\n",
        "\n",
        "# Print comparison\n",
        "print(\"\\nComparison with Sklearn Metrics:\")\n",
        "print(f\"Sklearn MAE: {sklearn_mae}\")\n",
        "print(f\"Sklearn MSE: {sklearn_mse}\")\n",
        "print(f\"Sklearn RMSE: {sklearn_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "RL_nxAeJJEU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Table 2: YActual vs. YPred (Classification Metrics)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Data from Table 2\n",
        "YActual2 = np.array([0, 0, 1, 1, 2, 0])\n",
        "YPred2 = np.array([0, 0, 1, 0, 2, 0])\n",
        "\n",
        "# Compute metrics from scratch\n",
        "accuracy_manual = np.sum(YActual2 == YPred2) / len(YActual2)\n",
        "\n",
        "# Precision, Recall, and F1 Score need computation per class\n",
        "unique_classes = np.unique(YActual2)\n",
        "precision_manual = []\n",
        "recall_manual = []\n",
        "f1_manual = []\n",
        "\n",
        "for cls in unique_classes:\n",
        "    tp = np.sum((YActual2 == cls) & (YPred2 == cls))\n",
        "    fp = np.sum((YActual2 != cls) & (YPred2 == cls))\n",
        "    fn = np.sum((YActual2 == cls) & (YPred2 != cls))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    precision_manual.append(precision)\n",
        "    recall_manual.append(recall)\n",
        "    f1_manual.append(f1)\n",
        "\n",
        "# Average metrics (macro average)\n",
        "precision_manual_avg = np.mean(precision_manual)\n",
        "recall_manual_avg = np.mean(recall_manual)\n",
        "f1_manual_avg = np.mean(f1_manual)\n",
        "\n",
        "# Compare with library outcomes\n",
        "accuracy_lib = accuracy_score(YActual2, YPred2)\n",
        "precision_lib = precision_score(YActual2, YPred2, average='macro', zero_division=0)\n",
        "recall_lib = recall_score(YActual2, YPred2, average='macro', zero_division=0)\n",
        "f1_lib = f1_score(YActual2, YPred2, average='macro', zero_division=0)\n",
        "\n",
        "print(\"Classification Metrics (Table 2):\")\n",
        "print(f\"Accuracy (Manual): {accuracy_manual}, Accuracy (Library): {accuracy_lib}\")\n",
        "print(f\"Precision (Manual): {precision_manual_avg}, Precision (Library): {precision_lib}\")\n",
        "print(f\"Recall (Manual): {recall_manual_avg}, Recall (Library): {recall_lib}\")\n",
        "print(f\"F1 Score (Manual): {f1_manual_avg}, F1 Score (Library): {f1_lib}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fesnxHiZHb-P",
        "outputId": "f75cf0db-c5db-4af1-a66e-4b613e666cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Metrics (Table 2):\n",
            "Accuracy (Manual): 0.8333333333333334, Accuracy (Library): 0.8333333333333334\n",
            "Precision (Manual): 0.9166666666666666, Precision (Library): 0.9166666666666666\n",
            "Recall (Manual): 0.8333333333333334, Recall (Library): 0.8333333333333334\n",
            "F1 Score (Manual): 0.8412698412698413, F1 Score (Library): 0.8412698412698413\n"
          ]
        }
      ]
    }
  ]
}